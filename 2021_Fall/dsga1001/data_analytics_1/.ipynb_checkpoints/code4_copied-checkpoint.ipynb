{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f21b19",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "movieRatingsDeidentified.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5eb78bbdf6b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# something Python can use: a matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'movieRatingsDeidentified.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_header\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;31m# We want just the data, so skip the first row / header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0mdataMatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstartColumn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstartColumn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnumMovies\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Should yield a n x 3 matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, like)\u001b[0m\n\u001b[1;32m   1789\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1791\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1792\u001b[0m             \u001b[0mfid_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    529\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: movieRatingsDeidentified.csv not found."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# ### Lab04\n",
    "# \n",
    "# ### 2021-09-23\n",
    "\n",
    "# ### Statistical significance tests on real data\n",
    "\n",
    "\n",
    "#%% In this script, we will analyze real data from a published article (Wallisch & Whritner 2017)\n",
    "# A subset of it. The point is specifically to illustrate hypothesis testing \n",
    "# in Python and to implement the Canonical Data Analysis cascade. \n",
    "# The advantage of this is that you'll be free of the limitations of canned\n",
    "# packages like Excel or SPSS. \n",
    "# Specifically, we will test the hypothesis that Matrix I was the best of\n",
    "# the Matrix movies (it's a trilogy). \n",
    "                    \n",
    "# Since we are doing this here, let me walk you through the null hypothesis\n",
    "# testing framework at least once (just once). It is a bit arcane. It made\n",
    "# sense to Fisher.                  \n",
    "                    \n",
    "# 1) Start with a hypothesis (something about the world you would wish to\n",
    "# know, whether it is true or not. The participants are just the unit of \n",
    "# analysis that gives you the data). \n",
    "\n",
    "# 2) State a null hypothesis and assume that it is 100% true (that there is\n",
    "# no difference in the conditions of 1), e.g. here Matrix I rated the same\n",
    "# as Matrix II and III). This is essential to NHST. \n",
    "\n",
    "# 3) This is - at the face of it - an odd thing to do, because naively you\n",
    "# would think that scientists look for probability (Hypothesis | Data) \n",
    "# But that is unknowable. Which is why we do the study in the first place\n",
    "# What is calculable: Probability (Data | NULL hypothesis)\n",
    "# You can assess the probability of the data given the null hypothesis\n",
    "\n",
    "# 4) To get this probability, we represent the sample by a parameter like a \n",
    "# sample mean, then transform the sample mean into a test statistic with a \n",
    "# known distribution.\n",
    "\n",
    "# 5) The area under the curve of the distribution of the test statistic in\n",
    "# the tail (or tails, if it is a 2-tailed test) is the p value, in other\n",
    "# words the probability of this result (or a more extreme one) given chance\n",
    "# alone.\n",
    "\n",
    "# 6) We compare the p value to a significance level alpha (typically 5% or 1%)\n",
    "\n",
    "# 7) Decision point (Choice)\n",
    "# a) If it is smaller than that, we decide to reject our assumption that the \n",
    "# null hypothesis is true.\n",
    "# b) If it is not smaller than that, we don't do anything because we already\n",
    "# assumed that the null hypothesis is true.                    \n",
    "                    \n",
    "# Paraphrased logic, in english: We acknowledge any outcome could be due to\n",
    "# chance. Our only question is how likely that is by chance. If it is\n",
    "# implausibly unlikely, we reject the assumption that it was just due to\n",
    "# chance. And either the null hypothesis is true or not, so if we reject it\n",
    "# is plausible, it probably means that our treatment did have an effect.                  \n",
    "                    \n",
    "#%% Before we do the stats, let's talk about the psychology of movie ratings:\n",
    "    \n",
    "# Hypothesis 1: First is best. Because that's usually true for trilogies\n",
    "# (with the exception of Star Wars). That's probably true just because of\n",
    "# regression to the mean. Also expectations. If the first one is great, it's\n",
    "# hard to beat expectations. \n",
    "\n",
    "# Hypothesis 2: Ratings for later ones will be even better. Production\n",
    "# qualities improve over time. Also, only fans might watch all of them.\n",
    "# People who didn't like the 1st one might drop out and never watch the\n",
    "# other ones, so the RATINGS might appear higher. \n",
    "\n",
    "# Make sure that you actually have 2 plausible outcomes before doing the\n",
    "# study. If the outcome is a foregone conclusion, it's not science. Science\n",
    "# is about being open to any possible outcome.\n",
    "\n",
    "# Null hypothesis: There is no difference. \n",
    "\n",
    "# Let's implement the canonical data analysis cascade\n",
    "\n",
    "#%% 0 Init (Birth)\n",
    "\n",
    "# a. Predispositions:\n",
    "startColumn = 177 # This is the column that contains data from matrix I\n",
    "# matrix II is column 178 and matrix III is column 179\n",
    "numMovies = 3 # Data from how many movies (it's a trilogy)\n",
    "pruningMode = 2 # This flag determines how we will remove nans. It's important to be mindful of this \n",
    "# 1 = element-wise, 2 = row-wise, 3 = imputation\n",
    "\n",
    "# b. Load/import - libraries/packages:\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%% 1 Loader / Transducer: \n",
    "# Taking the inputs from their native form and putting it into \n",
    "# something Python can use: a matrix\n",
    "\n",
    "data = np.genfromtxt('movieRatingsDeidentified.csv', delimiter = ',', skip_header = 1)\n",
    "# We want just the data, so skip the first row / header\n",
    "dataMatrix = data[:,startColumn:startColumn+numMovies] # Should yield a n x 3 matrix\n",
    "\n",
    "# Decompose data_matrix into separate arrays, one for each movie:\n",
    "M1 = dataMatrix[:,0]\n",
    "M2 = dataMatrix[:,1]\n",
    "M3 = dataMatrix[:,2]\n",
    "# They are all the same length because if the participant did not respond\n",
    "# the element is represented with NaN\n",
    "\n",
    "#%% 2 \"Thalamus\" stage: We need to get rid of \"bad\" data. \n",
    "\n",
    "# This is not data we don't like the results of. \n",
    "# It's data that if it entered the analysis stream would ruin our analysis\n",
    "\n",
    "# By visual inspection, the matrix of movie ratings contains numbers from 1\n",
    "# to 4, representing \"star ratings\", and \"nans\". \n",
    "# What could the nans be? What do they mean?\n",
    "# It could be people not doing the task. There are many ways to detect this.\n",
    "# People who press buttons not in the instructions, people responding too\n",
    "# fast, people responding too slow. \n",
    "\n",
    "# Why might \"too slow\" be a problem? In one study, we were interested in\n",
    "# reaction time as a dependent variable. But once, a participant went to the\n",
    "# restroom in the middle of the experiment and didn't come back for 30\n",
    "# minutes. What would happen to the mean reaction time for the condition\n",
    "# that the trial was in, in which this participant used the restroom. If we\n",
    "# included this, it would be meaningless. The mean is sensitive to outliers.\n",
    "# The rest of the reactions are in milli-seconds. This is 4 orders of\n",
    "# magnitude higher. Another example from neuroscience: Voltages of\n",
    "# electrodes are usually in mV or microV (depending on eeg vs.\n",
    "# microelectrode). What if for one trial there was voltage surge that hit\n",
    "# the building - all measurements from that trial are invalid. You need to\n",
    "# exclude that BEFORE looking at the results. All results will be thrown off\n",
    "# by that.\n",
    "\n",
    "# Given that these are movie ratings, it is not implausible that most of\n",
    "# these nans represent missing data due to the participant did not watch the\n",
    "# movie in question. \n",
    "# The reason we have to deal with this is that once this data is in the\n",
    "# analysis stream, it will make the rest of the interpretation hard. If\n",
    "# there is a single nan in the data, we can't take the mean. \n",
    "# So \"filtering\" or removing of ill-formed data will mean \"removal of\n",
    "# missing data\" in this case.\n",
    "# But here is the catch: Remove them how?\n",
    "\n",
    "# There are at least 3 ways to handle this, and depending on how we do this,\n",
    "# it will set up the entire rest of the analysis (and what it means). \n",
    "\n",
    "# 1) Element-wise removal of nans: We remove nans from each data matrix\n",
    "# where we find it. But we are starting with an equal n for each movie. 3204\n",
    "# rows in all 3 matrices. Once we start removing stuff element-wise, due to\n",
    "# the fact that there are an unequal number of nans for each movie, this\n",
    "# will result in unequal n, which will make some analyses impossible. Some\n",
    "# analyses presume equal n. It could also introduce bias. If we remove\n",
    "# people who have not seen the latter ones, but that was a choice (their\n",
    "# choice), we would inflate the ratings artificially. \n",
    "\n",
    "# 2) Row-wise (participant-wise) removal of nans: If a participant has not\n",
    "# seen even one of the movies, we remove all of the data from this\n",
    "# participant. Good: We keep n the same. Bad: We will lose a lot of data.\n",
    "# Probably most of the data. This usually looks suspicious, also: Loss of\n",
    "# statistical power. \n",
    "\n",
    "# 3) Imputation: We replace the missing data with our guess of what the \n",
    "# rating would have been, if there had been a rating. \n",
    "# This is more commonly done in engineering than in science. \n",
    "# Sometimes, people replace the missing value with the mean. But which mean?\n",
    "# The participant-wise mean or the movie-wise mean - or a blend of the two? \n",
    "# We could do the average of the movie, but is that fair? Those people chose\n",
    "# not to watch it - they were not randomly assigned to watch the movies.\n",
    "# This suggests that their rating would have been lower than that average.\n",
    "# But how much lower? 0? Probably too extreme. \n",
    "# In science, data only comes from measurement\n",
    "\n",
    "# In real life, pick the option that makes the most sense for your\n",
    "# theoretical approach, and just do that. For teaching purposes, we'll do\n",
    "# all of them, just so you see how that plays out in real life.\n",
    "\n",
    "# 1) Element-wise:\n",
    "if pruningMode == 1:\n",
    "   M1 = M1[np.isfinite(M1)] # only keep the finite elements (not infinity or NaN)\n",
    "   M2 = M2[np.isfinite(M2)] # only keep the finite elements (not infinity or NaN)\n",
    "   M3 = M3[np.isfinite(M3)] # only keep the finite elements (not infinity or NaN)\n",
    "# Outcome: Exactly as we suspected, n is unequal now, and we have concerns\n",
    "# about the psychological interpretation of this, as we only have the\n",
    "# ratings of die-hard fans who watched all 3 for the third one.\n",
    "\n",
    "# 2) Row wise:\n",
    "elif pruningMode == 2:\n",
    "    temp = np.array([np.isnan(M1),np.isnan(M2),np.isnan(M3)],dtype=bool)\n",
    "    temp2 = temp*1 # convert boolean to int\n",
    "    temp2 = sum(temp2) # take sum of each participant\n",
    "    missingData = np.where(temp2>0) # find participants with missing data\n",
    "    M1 = np.delete(M1,missingData) # delete missing data from array\n",
    "    M2 = np.delete(M2,missingData) # delete missing data from array\n",
    "    M3 = np.delete(M3,missingData) # delete missing data from array\n",
    "# Good: We have equal n now. \n",
    "# Bad: We only have 1493 out of 3204 we started with left\n",
    "# That is a) suspicious, b) we lost a lot of power\n",
    "\n",
    "#%% 3 Reformatting - V1: Format data into a representation to work with\n",
    "\n",
    "# I did most of the work for you already. In preprocessing. In other words,\n",
    "# I took the records that came in over time by participant and broke them\n",
    "# up, so they are now ordered by movie. I already abstracted over time.\n",
    "# There is still something we can do that will make our life easier, which\n",
    "# is to put this into a format we can loop over (to do all these tests). Why\n",
    "# is the current format not great? M1 vs. M3, etc. is not great for that?\n",
    "# Wouldn't it be better to have a matrix \"DATA\" that contains all 3 movies?\n",
    "# It would be. Why can't it be a matrix now, given we removed missing data\n",
    "# element-wise? Because n is unequal. So let's make it an array of arrays\n",
    "\n",
    "if pruningMode == 1:\n",
    "    combinedData = np.transpose(np.array([M1,M2,M3])) # array of arrays\n",
    "elif pruningMode == 2:\n",
    "    combinedData = np.transpose(np.array([M1,M2,M3])) # 2D array\n",
    "    # We can now put the data into a 2D array because we have an equal number\n",
    "    # of rows for M1, M2 and M3\n",
    "    \n",
    "#%% 4a) Extrastriate cortex: Doing the actual specialized analyses\n",
    "# Descriptive statistics - we are looking for very special numbers that\n",
    "# capture the essence of the entire dataset - the typical number (central\n",
    "# tendency) and the dispersion. Typically mean and SD. \n",
    "\n",
    "# Initialize container to store descriptives:\n",
    "descriptivesContainer = np.empty([numMovies,4])\n",
    "descriptivesContainer[:] = np.NaN \n",
    "\n",
    "# 1. Element-wise:\n",
    "if pruningMode == 1:\n",
    "    for ii in range(numMovies):\n",
    "        descriptivesContainer[ii,0] = np.mean(combinedData[ii]) # mu\n",
    "        descriptivesContainer[ii,1] = np.std(combinedData[ii]) # sigma\n",
    "        descriptivesContainer[ii,2] = len(combinedData[ii]) # n\n",
    "        descriptivesContainer[ii,3] = descriptivesContainer[ii,1]/np.sqrt(descriptivesContainer[ii,2]) # sem\n",
    "\n",
    "# 2. Row-wise:\n",
    "elif pruningMode == 2:\n",
    "    for ii in range(numMovies):\n",
    "        descriptivesContainer[ii,0] = np.mean(combinedData[:,ii]) # mu\n",
    "        descriptivesContainer[ii,1] = np.std(combinedData[:,ii]) # sigma\n",
    "        descriptivesContainer[ii,2] = len(combinedData[:,ii]) # n\n",
    "        descriptivesContainer[ii,3] = descriptivesContainer[ii,1]/np.sqrt(descriptivesContainer[ii,2]) # sem\n",
    "        \n",
    "#%% 4b) Extrastriate cortex part II: Inferential statistics\n",
    "\n",
    "# So far, so good. The mean rating of matrix 1 is higher than that of matrix\n",
    "# 2, and the mean of matrix 2 is higher than matrix 3. \n",
    "\n",
    "# The question here is whether the differences we saw in the descriptives -\n",
    "# usually the means - are \"statistically significant\". \n",
    "# Whether it is plausibly consistent with chance (that the numbers came out\n",
    "# of a RGN)\n",
    "\n",
    "# 1) Assert a null hypothesis: We assume that the data came out of a RNG -\n",
    "# strictly by chance.\n",
    "\n",
    "# 2) We compute the probability that this is the case - assuming chance. \n",
    "\n",
    "# 3) If this probability is implausibly low, we DECIDE to concede that we\n",
    "# were wrong in 1) - that it is probably not solely due to chance. \n",
    "\n",
    "# It's a choice, it's a decision. You have not falsified the null hypothesis\n",
    "# or \"proven\" to be wrong. We made a choice that it is implausible. But we\n",
    "# could be wrong (type I and type II errors). \n",
    "\n",
    "# In science, we consider things that happen only 1 in 20 times to be \"too\n",
    "# implausible\" to be consistent with chance. This corresponds to getting\n",
    "# heads (or tails) 5 times in a row (if you flip coins). Is that terribly\n",
    "# implausible? There is now a movement afoot to lower this to 1 in 200. To\n",
    "# avoid false positives. So results won't reproduce. This gave p values a\n",
    "# bad reputation. But there is nothing wrong with them, if they are\n",
    "# understood and used properly. Properly = conservative enough criterion of\n",
    "# implausibility and high enough power. \n",
    "\n",
    "# So our question now is: Assuming that there is no difference in reality, how\n",
    "# likely is it to get this mean difference just by chance (sampling error).    \n",
    "\n",
    "# Analogy: We are doing a crime scene investigation. Null hypothesis is the\n",
    "# presumption of innocence. We only reject that presumption if forced by the\n",
    "# data (in other words, if the evidence suggests that the fingerprints, DNA,\n",
    "# etc.) of the suspect didn't just get there by chance. It is always\n",
    "# possible. The unreasonable doubt (that your evidence got there just by\n",
    "# chance always persists). \n",
    "\n",
    "# We need to do a t-test because we want to assess how likely this observed\n",
    "# mean difference is and we do not know the population parameters. We have\n",
    "# to do an independent samples t-test because we eliminated missing data in a \n",
    "# way to yield unequal n. We have no choice (if we want to do a t-test)\n",
    "\n",
    "# 1. Element-wise:\n",
    "if pruningMode == 1:\n",
    "    t1,p1 = stats.ttest_ind(combinedData[0],combinedData[1])\n",
    "    # There is a significant difference. \n",
    "    # In english: The difference between the samples (specifically the sample \n",
    "    # means) is too large to be reasonably consistent with chance. \n",
    "\n",
    "    # Now let's compute the degrees of freedom (N1 + N2 - 2):\n",
    "    df = int(descriptivesContainer[0,2] + descriptivesContainer[1,2] - 2)\n",
    "\n",
    "    # How likely was our mean difference assuming chance? \n",
    "    # The p-value is really small. 5% is an extremely liberal threshold.\n",
    "    # p-values of real effects are very close to 0, if you have sufficient\n",
    "    # power. They are distributed as a beta-distribution. Like this one.\n",
    "\n",
    "    # Degrees of freedom for an independent samples t-test is:\n",
    "    # Sample size1 + sample size2 - 2 (2 because we calculate 2 sample means and\n",
    "    # we lose 1 df per sample mean). So the general equation for df is not n -\n",
    "    # 1, it is n - k. \n",
    "\n",
    "    # The independent samples t-test is the only one we can do, once we have\n",
    "    # unequal n, but is it the right test? Did the data about the 2 movies come\n",
    "    # from different populations? No. They are the same people. In reality, we\n",
    "    # probably have much less df. \n",
    "\n",
    "    # Mean of Matrix 2 and 3 seem rather close. Question: Plausibly due to chance\n",
    "    # (sampling error) alone?\n",
    "    t2,p2 = stats.ttest_ind(combinedData[1],combinedData[2])\n",
    "\n",
    "    # We can do as many t-tests as we want. We will have to adjust the\n",
    "    # alpha-level from 0.05 to 0.05/c, where c is the number of tests\n",
    "    # (comparisons), that's called the \"Bonferroni correction\". \n",
    "\n",
    "    # Even this comparison is significant, even after the Bonferroni correction,\n",
    "    # so we conclude that this observed difference in ratings is not due to\n",
    "    # chance alone. In other words, Matrix 2 is a better movie than Matrix 3. \n",
    "    # Expectation: The p-value will be even lower than 1 vs. 2, because the\n",
    "    # mean difference is larger, but it might not be, because the df will be\n",
    "    # less (fewer people saw M3). The p value comes from the t-value in light of\n",
    "    # the df (because the t-distribution is a family of distributions which\n",
    "    # differs as df - that's a parameter).\n",
    "    t3,p3 = stats.ttest_ind(combinedData[0],combinedData[2])\n",
    "\n",
    "    # Do this with ANOVA - allows to compare more than 2 sample means without \n",
    "    # inflating alpha (from multiple comparisons)\n",
    "    f,p = stats.f_oneway(combinedData[0],combinedData[1],combinedData[2])\n",
    "    \n",
    "    # Nonparametric tests equivalent to t-tests - Mann-Whitney U test:\n",
    "    # Test for comparing medians of ordinal data (such as movie ratings)\n",
    "    # from 2 groups\n",
    "    u1,p1 = stats.mannwhitneyu(combinedData[0],combinedData[1])\n",
    "    u2,p2 = stats.mannwhitneyu(combinedData[0],combinedData[2])\n",
    "    u3,p3 = stats.mannwhitneyu(combinedData[1],combinedData[2])\n",
    "\n",
    "    # Nonparametric tests equivalent to ANOVA - Kruskal-Wallis:\n",
    "    # Same assumptions as above, but for more than 2 groups\n",
    "    h,p = stats.kruskal(combinedData[0],combinedData[1])\n",
    "    \n",
    "    \n",
    "\n",
    "# These are not different people. So we have to do a \"within subjects\"\n",
    "# design (repeated measures) - in other words do a paired samples t-test. \n",
    "# But to do that, we need to do row-wise removal of missing data \n",
    "\n",
    "# 2. Row-wise:\n",
    "elif pruningMode == 2:\n",
    "    # Because our n's are now equal, we can now do a t-test for dependent\n",
    "    # groups (which is much more appropriate, because our data came from the\n",
    "    # *same* people, the groups WERE dependent, we previously inflated df)\n",
    "    t1,p1 = stats.ttest_rel(combinedData[:,0],combinedData[:,1])\n",
    "    df = len(combinedData) - 1\n",
    "    # Df HERE is now n-1, because the t-test for dependent groups first\n",
    "    # converts all scores to differences, then takes ONE mean, so we only\n",
    "    # lose 1 df. Overall, we have much fewer df, than in the independent\n",
    "    # samples t-test.\n",
    "    # The p-value is even lower. How is that possible, given that df is\n",
    "    # lower too? \n",
    "    # In an independent samples t-test individual differences (some people\n",
    "    # just don't like anything, or everything) all goes into the\n",
    "    # denominator, in other words is interpreted as error. That lowers the\n",
    "    # t-value. In a paired-samples t-test, everyone is their own control. So\n",
    "    # the individual differences go away. In english: A paired samples\n",
    "    # t-test is usually much more powerful, even if the df is lower. So you\n",
    "    # should do this one, whenever you can. \n",
    "    t2,p2 = stats.ttest_rel(combinedData[:,1],combinedData[:,2])\n",
    "    t3,p3 = stats.ttest_rel(combinedData[:,0],combinedData[:,2])\n",
    "    \n",
    "    # Let's do an ANOVA instead:\n",
    "    f,p = stats.f_oneway(combinedData[:,0],combinedData[:,1],combinedData[:,2])\n",
    "    \n",
    "#%% 5 Motor cortex: Plot the data \n",
    "# Here we are going to plot the data from our ANOVA and add a title\n",
    "# that contains the f- and p-values\n",
    "\n",
    "# First, let's run our ANOVA once again:\n",
    "if pruningMode == 1:\n",
    "    f,p = stats.f_oneway(combinedData[0],combinedData[1],combinedData[2])\n",
    "elif pruningMode == 2:\n",
    "    f,p = stats.f_oneway(combinedData[:,0],combinedData[:,1],combinedData[:,2])\n",
    "    \n",
    "# Now, let's plot it:\n",
    "x = ['Matrix 1', 'Matrix 2', 'Matrix 3'] # labels for the bars\n",
    "xPos = np.array([1,2,3]) # x-values for the bars\n",
    "plt.bar(xPos,descriptivesContainer[:,0],width=0.5,yerr=descriptivesContainer[:,3]) # bars + error  \n",
    "plt.xticks(xPos, x) # label the x_pos with the labels\n",
    "plt.ylabel('Mean rating') # add y-label\n",
    "plt.title('f = {:.3f}'.format(f) + ', p = {:.3f}'.format(p)) # title is the test stat and p-value\n",
    "# Note: we round our p-value to the nearest thousandth, which in our case is 0.000\n",
    "# In reality, it is much smaller than that. Instead, we could have chosen to use\n",
    "# scientific notation in our title. More on that later.\n",
    "\n",
    "#%% Supplementary material 1: Representing the data with a Pandas dataframe and doing a 2-way ANOVA in Python and make an ANOVA table (and a means plot)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load data:\n",
    "df = pd.read_csv('movieRatingsDeidentified.csv',skipinitialspace=True)\n",
    "# Fill empty strings with NaN\n",
    "# Now we have the headers AND the data in one object\n",
    "# This is a DataFrame. For handling tabular data.\n",
    "\n",
    "# 2. Let's get a handle on our movie titles:\n",
    "titles = df.columns \n",
    "print(titles)\n",
    "# We won't use this for subsequent analyses, but it's nice to see all the titles at once\n",
    "\n",
    "# 3. Find the Matrix data:\n",
    "title = 'Matrix' # or any other title, for that matter\n",
    "theMatrix = df.loc[:,df.columns.str.contains(title)]\n",
    "\n",
    "# 4. Perform descriptives:\n",
    "magic = theMatrix.describe()\n",
    "# We don't have to run a loop or initialize a container\n",
    "# We are still missing the SEM, so let's add it:\n",
    "temp = magic.iloc[2,:]/np.sqrt(magic.iloc[0,:])\n",
    "magic.loc['sem'] = temp\n",
    "\n",
    "#%% Supplementary material 2: Do a two-way ANOVA and show an ANOVA table\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.graphics.factorplots import interaction_plot as meansPlot\n",
    "\n",
    "df = pd.read_csv('twoWayAnovaExample.csv',skipinitialspace=True) # load new data set\n",
    "df.info() # What is the structure of the data frame?\n",
    "\n",
    "model = ols('Value ~ X1 + X2 + X1:X2', data=df).fit() #Build the two-way ANOVA model. Value = y, X1,X2 = Main effects. X1:X2 = interaction effect\n",
    "anova_table = sm.stats.anova_lm(model, typ=2) #Create the ANOVA table. Residual = Within\n",
    "print(anova_table) #Show the ANOVA table\n",
    "\n",
    "#Show the corresponding means plot\n",
    "fig = meansPlot(x=df['X1'], trace=df['X2'], response=df['Value'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd2aa40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
