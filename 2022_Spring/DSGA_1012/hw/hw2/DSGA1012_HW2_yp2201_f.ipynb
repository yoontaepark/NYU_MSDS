{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBnmbg-jdztQ"
   },
   "source": [
    "# HW2: Spam classification with LSTM\n",
    "\n",
    "The deadline is **9:30 am Feb 16, 2022**.   \n",
    "You should submit a `.ipynb` file with your solutions to NYU Brightspace.\n",
    "\n",
    "---\n",
    "\n",
    "In this homework, we will reuse the spam prediction dataset used in HW1.\n",
    "We will use a word-level BiLSTM sentence encoder to encode the sentence and a neural network classifier.\n",
    "\n",
    "For reference, you may read [this paper](https://arxiv.org/abs/1705.02364).\n",
    "\n",
    "Lab 3 is especially relevant to this homework.\n",
    "\n",
    "## Points distribution\n",
    "\n",
    "1. code `spam_collate_func`: 25 pts\n",
    "2. code `LSTMClassifier.init`: 25 pts\n",
    "3. code `LSTMClassifier.forward`: 20 pts\n",
    "4. code `evaluate`: 10 pts\n",
    "5. code for training loop: 10 pts\n",
    "6. Question on early stopping: 10 pts\n",
    "\n",
    "How we grade the code: \n",
    "- full points if code works and the underlying logic is correct;\n",
    "- half points if code works but the underlying logic is incorrect;\n",
    "- zero points if code does not work.\n",
    "\n",
    "Therefore, **make sure your code works, i.e., no error is being produced when you execute the code.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKJv-b6RewJn"
   },
   "source": [
    "# Data Loading\n",
    "First, reuse the code from HW1 to download and read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QoiahW1_fZ6p",
    "outputId": "59ff18dd-9a25-4f7c-a1e1-4b56dd2c0a5e"
   },
   "outputs": [],
   "source": [
    "# !wget 'https://docs.google.com/uc?export=download&id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR' -O spam.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "I52OxyBgfi_j",
    "outputId": "82bf1775-4161-4bca-e58f-3e4edeff5f0d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   v1                                                 v2\n",
       "0   0  Go until jurong point, crazy.. Available only ...\n",
       "1   0                      Ok lar... Joking wif u oni...\n",
       "2   1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   0  U dun say so early hor... U c already then say...\n",
       "4   0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"spam.csv\", usecols=[\"v1\", \"v2\"], encoding='latin-1')\n",
    "# 1 - spam, 0 - ham\n",
    "df.v1 = (df.v1 == \"spam\").astype(\"int\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCihb3oogn27"
   },
   "source": [
    "We will split the data into train, val, and test sets.  \n",
    "`train_texts`, `val_texts`, and `test_texts` should contain a list of text examples in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0H78E3FLgEA2",
    "outputId": "f5f3e061-908d-45d1-977e-29ec815298ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of initial data: 5572\n",
      "Train size: 3902\n",
      "Val size: 835\n",
      "Test size: 835\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0.15 for val, 0.15 for test, 0.7 for train\n",
    "val_size = int(df.shape[0] * 0.15)\n",
    "test_size = int(df.shape[0] * 0.15)\n",
    "\n",
    "# Shuffle the data (sample all rows without replacement)\n",
    "df = df.sample(frac=1)\n",
    "# Split df to test/val/train\n",
    "test_df = df[:test_size]\n",
    "val_df = df[test_size:test_size+val_size]\n",
    "train_df = df[test_size+val_size:]\n",
    "\n",
    "\n",
    "train_texts, train_labels = list(train_df.v2), list(train_df.v1)\n",
    "val_texts, val_labels     = list(val_df.v2), list(val_df.v1)\n",
    "test_texts, test_labels   = list(test_df.v2), list(test_df.v1)\n",
    "\n",
    "\n",
    "# Check that idces do not overlap\n",
    "assert set(train_df.index).intersection(set(val_df.index)) == set({})\n",
    "assert set(test_df.index).intersection(set(train_df.index)) == set({})\n",
    "assert set(val_df.index).intersection(set(test_df.index)) == set({})\n",
    "# Check that all idces are present\n",
    "assert df.shape[0] == len(train_labels) + len(val_labels) + len(test_labels)\n",
    "\n",
    "# Sizes\n",
    "print(\n",
    "    f\"Size of initial data: {df.shape[0]}\\n\"\n",
    "    f\"Train size: {len(train_labels)}\\n\"\n",
    "    f\"Val size: {len(val_labels)}\\n\"\n",
    "    f\"Test size: {len(test_labels)}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FX8D130ngVxu",
    "outputId": "625cb734-6688-483c-f436-4f0309f6239a"
   },
   "outputs": [],
   "source": [
    "# train_texts[:10]  # Just checking the examples in train_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Sm_iuR_hJp2"
   },
   "source": [
    "# Download and Load GloVe Embeddings\n",
    "We will use GloVe embedding parameters to initialize our layer of word representations / embedding layer.\n",
    "Let's download and load glove.\n",
    "\n",
    "\n",
    "This is related Lab 3 Deep Learning, please watch the recording and check the notebook for details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRCcCtcSjEPR",
    "outputId": "76612afe-785f-4e27-f720-2b344de2fb48",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#@title Download GloVe word embeddings\n",
    "\n",
    "# === Download GloVe word embeddings\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "# === Unzip word embeddings and use only the top 50000 word embeddings for speed\n",
    "# !unzip glove.6B.zip\n",
    "# !head -n 50000 glove.6B.300d.txt > glove.6B.300d__50k.txt\n",
    "\n",
    "# === Download Preprocessed version\n",
    "# !wget 'https://docs.google.com/uc?id=1KMJTagaVD9hFHXFTPtNk0u2JjvNlyCAu' -O glove_split.aa\n",
    "# !wget 'https://docs.google.com/uc?id=1LF2yD2jToXriyD-lsYA5hj03f7J3ZKaY' -O glove_split.ab\n",
    "# !wget 'https://docs.google.com/uc?id=1N1xnxkRyM5Gar7sv4d41alyTL92Iip3f' -O glove_split.ac\n",
    "# !cat glove_split.?? > 'glove.6B.300d__50k.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AfN4rYTOmCD"
   },
   "source": [
    "## Load GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TSF0C4jHjnSz",
    "outputId": "1b66f611-0258-4276-a1f6-eba302b18ecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "def load_glove(glove_path, embedding_dim):\n",
    "    with open(glove_path) as f:\n",
    "        token_ls = [PAD_TOKEN, UNK_TOKEN]\n",
    "        embedding_ls = [np.zeros(embedding_dim), np.random.rand(embedding_dim)]\n",
    "        for line in f:\n",
    "            token, raw_embedding = line.split(maxsplit=1)\n",
    "            token_ls.append(token)\n",
    "            embedding = np.array([float(x) for x in raw_embedding.split()])\n",
    "            embedding_ls.append(embedding)\n",
    "        embeddings = np.array(embedding_ls)\n",
    "        print(embedding_ls[-1].size)\n",
    "    return token_ls, embeddings\n",
    "\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "EMBEDDING_DIM = 300 # dimension of Glove embeddings\n",
    "glove_path = \"glove.6B.300d__50k.txt\"\n",
    "vocab, embeddings = load_glove(glove_path, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50002, (50002, 300))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50002"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50002, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape[0], embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_VZkGbgO4yA"
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FpbnKsQeptXw",
    "outputId": "f9f3c005-21af-4b01-ed3e-495e79e6539b"
   },
   "outputs": [],
   "source": [
    "# !pip install sacremoses\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sacremoses\n",
    "from torch.utils.data import dataloader, Dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Wwy3gSvO87p"
   },
   "source": [
    "# Tokenize text data.\n",
    "We will use the `tokenize` function to convert text data into sequence of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "66d27ecd0c12421c8295182d4b1c343d",
      "171f4eaed9bd4123ae3b3463a8a8b642",
      "f5cd07efca614a8586ea13a82f109290",
      "e39a005b3c1f4f00a7ce74a023bb6fa9",
      "6c2c857c8a8b4e29a09f713a83771f0d",
      "a8a08f68d96046c088a55f5e3863f47b",
      "010317c9128449e1b0b7b536b6091576",
      "b3483370c5c541b9ae710e0e576ad7a8",
      "25e171acd5a84aa8b4de8aee01e3ccbd",
      "2c9b2d4bfaa34f69a180b4bc948f014e",
      "69ef9d2c6b5b492db9e33e48b0193f37",
      "658dc5d1efc840cba2450740eaac9773",
      "2f5b52b014f2439b905fa700bf636517",
      "c62fce4a8295479c83a928b79db89861",
      "e335b3c83bc1446f9e690ef5aa19bccf",
      "75e7a76026cd48e7b2b0575adff5395e",
      "8753674ef7f64029aae6f34d6e933f39",
      "18454ca7c2d749cbac54ade9cc8a1ce3",
      "306699c7eb094435a950180873effe60",
      "f6ea322a86ac4e70a57834232dfa6bab",
      "48fbfa8e945f4e0989be39b6ff0247fc",
      "424e07bfbd71417dbff02a70b4774aeb",
      "8c817714e1a446dfb84717a18d82cbae",
      "97cdbcc74cb24fb9a4ec08187b08e019",
      "5f4c4954e0c940c381d6aba07b72d67b",
      "5e3506a73adc4921bf8b2b110eb7a30a",
      "70ede22c755a49d28e1803b112b973b8",
      "ccc80287cfb84c4e88600fd970cb3a08",
      "819d692e3f61490e95bf3d289138ca85",
      "3d7fdd472f9d4e1a94f8afedfdb69dda",
      "bbd525ba97374d278314653168c08846",
      "a8028e07b39743f699237568579affb3",
      "39ad225c55a7418a8393e5d5217392f4"
     ]
    },
    "id": "j1aLbeOBmRyR",
    "outputId": "09caf839-faac-416f-a973-fb81d12934ea",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe6691a8cdd423cba51d221b092fa57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3902 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01e0b99018142c58846925c5976dd31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af1984eb49846488e57f38669b0790a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(data, labels, tokenizer, vocab, max_seq_length=128):\n",
    "    vocab_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "    text_data = []\n",
    "    label_data = []\n",
    "    for ex in tqdm(data):\n",
    "        tokenized = tokenizer.tokenize(ex.lower())\n",
    "        ids = [vocab_to_idx.get(token, 1) for token in tokenized]\n",
    "        text_data.append(ids)\n",
    "    return text_data, labels\n",
    "\n",
    "tokenizer = sacremoses.MosesTokenizer()\n",
    "\n",
    "train_data_indices, train_labels = tokenize(train_texts, train_labels, tokenizer, vocab)\n",
    "val_data_indices, val_labels = tokenize(val_texts, val_labels, tokenizer, vocab)\n",
    "test_data_indices, test_labels = tokenize(test_texts, test_labels, tokenizer, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEF48Bddt5kA",
    "outputId": "b72bceb3-2562-4892-85c2-c1b51c52830d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train text first 5 examples:\n",
      " [[1, 9600, 43, 17, 2712, 12, 9, 7301], [52, 2316, 47, 4021, 1892, 1970, 796, 3, 36, 7235, 2, 1, 102, 46770, 4119, 19, 10576, 1097, 122, 1, 4, 4, 4, 3363, 103, 725, 8459, 91, 205, 3, 43, 1, 692, 3954, 4, 807], [43, 3318, 1, 35, 4354, 1, 225, 3, 43, 151, 6016, 1, 806, 62, 9, 1, 181, 6, 1714, 3793], [102, 8237, 161, 22, 279, 48, 185, 21, 534, 9760, 4], [1, 1, 8609, 1, 1, 145, 1, 192, 759, 1, 1, 5561, 1, 1, 1]]\n",
      "\n",
      "Train labels first 5 examples:\n",
      " [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTrain text first 5 examples:\\n\", train_data_indices[:5])\n",
    "print(\"\\nTrain labels first 5 examples:\\n\", train_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dlUYNvgPUXs"
   },
   "source": [
    "# Create DataLoaders (25 pts)\n",
    " Now, let's create pytorch DataLoaders for our train, val, and test data.\n",
    "\n",
    " `SpamDataset` class is based on torch [`Dataset`](https://pytorch.org/docs/1.7.0/data.html?highlight=dataset#torch.utils.data.Dataset). It has an additional parameter called `self.max_sent_length` and a `spam_collate_func`.\n",
    "\n",
    "In order to use batch processing, all the examples need to effectively be the same length. We'll do this by adding padding tokens. `spam_collate_func` is supposed to dynamically pad or trim the sentences in the batch based on `self.max_sent_length` and the length of longest sequence in the batch. \n",
    "- If `self.max_sent_length` is less than the length of longest sequence in the batch, use `self.max_sent_length`. Otherwise, use the length of longest sequence in the batch.\n",
    "- We do this because our input sentences in the batch may be much shorter than `self.max_sent_length`.  \n",
    "\n",
    "Please check the comment block in the code near TODO for more details.\n",
    "\n",
    "\n",
    "Example: \n",
    "\n",
    "* PAD token id = 0\n",
    "* max_sent_length = 5\n",
    "\n",
    "input list of sequences:\n",
    "```\n",
    "inp = [\n",
    "    [1,4,5,3,5,6,7,4,4],\n",
    "    [3,5,3,2],\n",
    "    [2,5,3,5,6,7,4],\n",
    "]\n",
    "```\n",
    "then padded minibatch looks like this:\n",
    "```\n",
    "padded_input = \n",
    "    [[1,4,5,3,5],\n",
    "     [3,5,3,2,0],\n",
    "     [2,5,3,5,6]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-uJDfnVMxBsz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list, max_sent_length=128):\n",
    "        \"\"\"\n",
    "        @param data_list: list of data tokens \n",
    "        @param target_list: list of data targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        self.max_sent_length = max_sent_length\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key, max_sent_length=None):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        if max_sent_length is None:\n",
    "            max_sent_length = self.max_sent_length\n",
    "        token_idx = self.data_list[key][:max_sent_length]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, label]\n",
    "\n",
    "    def spam_collate_func(self, batch):\n",
    "        \"\"\"\n",
    "        Customized function for DataLoader that dynamically pads the batch so that all \n",
    "        data have the same length\n",
    "        # What the input `batch`? That's for you to figure out!\n",
    "        # You can read the Dataloader documentation, or you can use print\n",
    "        # function to debug. \n",
    "        \"\"\" \n",
    "        data_list = [] # store padded sequences\n",
    "        label_list = []\n",
    "        \n",
    "        # the length of longest sequence in batch\n",
    "        # if it is less than self.max_sent_length\n",
    "        # else max_batch_seq_len = self.max_sent_length\n",
    "        \n",
    "        data_indice_list = []\n",
    "        \n",
    "        for data_indice, label_index in batch:\n",
    "            data_indice_list.append(len(data_indice))\n",
    "        \n",
    "        max_batch_seq_len = min(max(data_indice_list), self.max_sent_length)\n",
    "\n",
    "        \"\"\"\n",
    "        Pad the sequences in your data \n",
    "        if their length is less than max_batch_seq_len\n",
    "        or trim the sequences that are longer than self.max_sent_length  \n",
    "        return padded data_list and label_list\n",
    "        1. TODO: Your code here \n",
    "         \"\"\"\n",
    "    \n",
    "        # Pad the sequences in your data \n",
    "        for data_index, label_index in batch:\n",
    "            if len(data_index) < max_batch_seq_len:\n",
    "                data_index = np.pad(data_index, (0, max_batch_seq_len - len(data_index)), 'constant', constant_values=0)\n",
    "                \n",
    "            else:\n",
    "                data_index = data_index[:max_batch_seq_len]\n",
    "            \n",
    "            data_list.append(data_index)\n",
    "            label_list.append(label_index)\n",
    "    \n",
    "        data_list = torch.tensor(np.array(data_list))\n",
    "        label_list = torch.tensor(np.array(label_list))\n",
    "        \n",
    "        return [data_list, label_list]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "max_sent_length=128\n",
    "train_dataset = SpamDataset(train_data_indices, train_labels, max_sent_length)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=train_dataset.spam_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = SpamDataset(val_data_indices, val_labels, train_dataset.max_sent_length)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=train_dataset.spam_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_dataset = SpamDataset(test_data_indices, test_labels, train_dataset.max_sent_length)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=train_dataset.spam_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68]) torch.Size([68]) torch.Size([68])\n"
     ]
    }
   ],
   "source": [
    "data_batch, labels = next(iter(train_loader))\n",
    "print(data_batch[0].shape, data_batch[1].shape, data_batch[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWgRGaCWf4Zz"
   },
   "source": [
    "Let's try to print out an batch from train_loader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "5O8R_KhwxULI",
    "outputId": "0406050e-f8b9-4c62-954a-0b6e21209d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch dimension:  torch.Size([64, 65])\n",
      "data_batch:  tensor([[  366,   366,     3,  ...,     0,     0,     0],\n",
      "        [ 3204,  8740,    43,  ...,     0,     0,     0],\n",
      "        [    1,    62, 10576,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  199,    34,    83,  ...,     0,     0,     0],\n",
      "        [  411, 42256,  3280,  ...,     0,     0,     0],\n",
      "        [   88,     9,    38,  ...,     0,     0,     0]])\n",
      "labels:  tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "data_batch, labels = next(iter(train_loader))\n",
    "print(\"data batch dimension: \", data_batch.size())\n",
    "print(\"data_batch: \", data_batch)\n",
    "print(\"labels: \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_batch:  tensor([ 366,  366,    3,  255,   83, 4004,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "print(\"data_batch: \", data_batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpdnYbPIgNXw"
   },
   "source": [
    "# Build a BiLSTM Classifier (20 + 25 + 10 pts)\n",
    "\n",
    "Now we are going to build a BiLSTM classifier. Check this [blog post](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) and [`torch.nn.LSTM`](https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM) for reference. Recall that we've also seen LSTM in Lab 3. \n",
    "\n",
    "The hyperparameters for LSTM are already given, but they are not necessarily optimal. You should get a good accuracy with these hyperparameters but you may try to tune the hyperparameters and use different hyperparameters to get better performance.\n",
    "\n",
    "* `__init__`: Class constructor. Here we define layers / parameters of LSTM.\n",
    "* `forward`: This function is used whenever you call your object as `model()`. It takes the input minibatch and returns the output representation from LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ModelBiLSTM(nn.Module):\n",
    "#   def __init__(self, options):\n",
    "#     # All the parameters to the model class & all the layers, go into __init__.\n",
    "#     # We define the layers here, and we **call** the layers later, in the\n",
    "#     # forward() function.\n",
    "#     super(ModelBiLSTM, self).__init__()\n",
    "# x(defined outside)    self.device = args['device']\n",
    "# x    self.embed_dim = args['embed_dim']  # using 100-dim GloVe\n",
    "# x    self.hidden_size = args['hidden_size']\n",
    "# x    self.num_layers = args['num_layers']\n",
    "# x    self.embedding = nn.Embedding.from_pretrained(\n",
    "# x     torch.load('/content/drive/My Drive/colabs/prep-lab3-nli/.vector_cache/multinli_vectors.pt'))  # modify to your path \n",
    "# x    self.directions = 2\n",
    "\n",
    "#     # Layers below: ReLU, dropout, projection (from embedding to input-to-LSTM),\n",
    "#     # LSTM, linear layers.\n",
    "# x    self.relu = nn.LeakyReLU()  # non-linear layer\n",
    "# x    self.dropout = nn.Dropout(p=args['dropout'])  # prob of an elt to be zeroed\n",
    "\n",
    "x   self.lstm = nn.LSTM(  # LSTM layer; Q: why put it here, within __init__?\n",
    "      self.embed_dim, self.hidden_size, self.num_layers,\n",
    "      dropout=args['dropout'], bidirectional=True,\n",
    "      batch_first=True)  # BATCH FIRST!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "#     self.linear_first = nn.Linear(\n",
    "#       self.hidden_size * self.directions * 4, self.hidden_size)  # why 4?\n",
    "#     self.linear_second = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "#     self.linear_third = nn.Linear(self.hidden_size, options['out_dim'])\n",
    "#     for layer in [self.linear_first, self.linear_second, self.linear_third]:\n",
    "#       nn.init.xavier_uniform_(layer.weight)\n",
    "#       nn.init.zeros_(layer.bias)\n",
    "        \n",
    "#     # this is the linear classification layers\n",
    "#     # with nonlinearity in the middle\n",
    "# x    self.layers = nn.Sequential(  \n",
    "#   \t  self.linear_first, self.relu, self.dropout,\n",
    "#       self.linear_second, self.relu, self.dropout,\n",
    "#   \t  self.linear_third,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   # In this part, remove premise functions to only use hypothesis\n",
    "# #   def forward(self, batch):\n",
    "#     # This is the forward pass of the our model.\n",
    "\n",
    "#     # ==========================================================================\n",
    "\n",
    "#     # STEP 1 (part 2): batched inputs -> embedded input (which will be fed into LSTM)\n",
    "\n",
    "#     # Q: shape of batch.premise (which is the token inputs)? batch_size * input_len_premise\n",
    "#     # Q: what does batch.premise[i][j] mean?\n",
    "\n",
    "#     # Q: shape of premise_embed (which will be fed into LSTM layers)?\n",
    "\n",
    "#     # premise_embed = self.embedding(batch.premise)\n",
    "#     hypothesis_embed = self.embedding(batch.hypothesis)\n",
    "\n",
    "#     # premise_embed.shape: batch size * premise input len * embed dim\n",
    "\n",
    "#     # ==========================================================================\n",
    "\n",
    "#     # STEP 2: LSTM\n",
    "#     # Q: what are the parameters to LSTM (when we're defining the LSTM)?\n",
    "#     # Q: what are the inputs to the LSTM layers?\n",
    "#     # Q: what are the outputs from the LSTM layers?\n",
    "#     # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "\n",
    "#     # Q: what is self.lstm?\n",
    "#     # If we write nn.LSTM(...)(premise_embed_proj) and\n",
    "#     # nn.LSTM(...)(hypothesis_embed_proj), what will be different?\n",
    "#     # premise_out, (premise_ht, _) = self.lstm(premise_embed, None)\n",
    "#     hypothesis_out, (hypothesis_ht, _) = self.lstm(hypothesis_embed, None)\n",
    "    \n",
    "#     # Q: what is the shape of premise_out and premise_ht?\n",
    "\n",
    "#     # Equivalently, the above code can be written as\n",
    "#     # h0 = torch.zeros((self.num_layers * self.directions, batch.batch_size,\n",
    "#     #                   self.hidden_size)).to(self.device)\n",
    "#     # c0 = torch.zeros((self.num_layers * self.directions, batch.batch_size,\n",
    "#     #                   self.hidden_size)).to(self.device)\n",
    "#     # premise_out, (premise_ht, _) = self.lstm(premise_embed, (h0, c0))\n",
    "#     # hypothesis_out, (hypothesis_ht, _) = self.lstm(hypothesis_embed, (h0, c0))    \n",
    "\n",
    "#     # ==========================================================================\n",
    "\n",
    "#     # STEP 3: linear layers for classification\n",
    "\n",
    "#     # Q: how do we convert premise_out and hypothesis_out into the inputs to linear layers?\n",
    "#     # Q: how do we design the linear layers?\n",
    "\n",
    "#     # premise = premise_out[:, -1, :]\n",
    "#     hypothesis = hypothesis_out[:, -1, :]\n",
    "\n",
    "#     # in combination, remove effect of premise\n",
    "#     combined = torch.cat(\n",
    "#       (hypothesis,  # Q: shape?\n",
    "#        torch.abs(hypothesis),\n",
    "#        hypothesis),  # Q: shape?\n",
    "\n",
    "#     # combined = torch.cat(\n",
    "#     #   (premise,  # Q: shape? A: batch_size * (num_directions * hidden_size)\n",
    "#     #    hypothesis,  # Q: shape?\n",
    "#     #    torch.abs(premise - hypothesis),\n",
    "#     #    premise * hypothesis),  # Q: shape?\n",
    "#       1)  # Q: what are we doing here and what's 1?\n",
    "\n",
    "#     # Q: what's the shape of combined? \n",
    "\n",
    "#     return self.layers(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTMClassifier classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings, hidden_size, num_layers, num_classes, bidirectional, dropout_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = self.load_pretrained_embeddings(embeddings)\n",
    "#         print(self.embedding_layer)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        self.embed_dim = embeddings.shape[1] #300\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.directions = 2\n",
    "        \n",
    "        self.non_linearity = nn.LeakyReLU() # For example, ReLU\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.embed_dim, self.hidden_size, self.num_layers, dropout=0.3, \\\n",
    "                            bidirectional=True, batch_first=True) \n",
    "   \n",
    "        self.linear_first = nn.Linear(self.hidden_size * self.directions * 4, self.hidden_size) \n",
    "        self.linear_second = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.linear_third = nn.Linear(self.hidden_size, 3)\n",
    "        \n",
    "        for layer in [self.linear_first, self.linear_second, self.linear_third]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "            \n",
    "        self.clf = nn.Sequential(  \n",
    "            self.linear_first, self.non_linearity, self.dropout,\n",
    "            self.linear_second, self.non_linearity, self.dropout,\n",
    "            self.linear_third,\n",
    "        ) # classifier layer\n",
    "        \n",
    "        \"\"\"\n",
    "           Define the components of your BiLSTM Classifier model\n",
    "           2. TODO: Your code here\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        \"\"\"\n",
    "           The code for loading embeddings from Lab 3 Deep Learning\n",
    "           Unlike lab, we are not setting `embedding_layer.weight.requires_grad = False`\n",
    "           because we want to finetune the embeddings on our data\n",
    "        \"\"\"\n",
    "        embedding_layer = nn.Embedding(embeddings.shape[0], embeddings.shape[1], padding_idx=0)\n",
    "        embedding_layer.weight.data = torch.Tensor(embeddings).float()\n",
    "        return embedding_layer\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        logits = None\n",
    "        \"\"\"\n",
    "           Write forward pass for LSTM. You must use dropout after embedding\n",
    "           the inputs. \n",
    "\n",
    "           Example, forward := embedding -> bilstm -> pooling (sum?mean?max?) \n",
    "                              nonlinearity -> classifier\n",
    "           Refer to: https://arxiv.org/abs/1705.02364\n",
    "\n",
    "           Return logits\n",
    "\n",
    "           3. TODO: Your code here\n",
    "        \"\"\"\n",
    "        \n",
    "        # self.embedding = nn.Embedding.from_pretrained(\n",
    "        # torch.load('/content/drive/My Drive/colabs/prep-lab3-nli/.vector_cache/multinli_vectors.pt'))\n",
    "        \n",
    "        \n",
    "        # STEP 1: batched inputs -> embedded input\n",
    "        inputs_embed = self.embedding_layer(inputs)\n",
    "        \n",
    "#         # STEP 2: LSTM\n",
    "        inputs_out, (inputs_ht, _) = self.lstm(inputs_embed, None)\n",
    "                \n",
    "# #         # STEP 3: linear layers for classification\n",
    "# #         inputs = inputs_out[:, -1, :]\n",
    "        \n",
    "# #         logits = self.clf(inputs)\n",
    "        \n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GOYEmADNDVIh"
   },
   "outputs": [],
   "source": [
    "# # First import torch related libraries\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class LSTMClassifier(nn.Module):\n",
    "#     \"\"\"\n",
    "#     LSTMClassifier classification model\n",
    "#     \"\"\"\n",
    "#     def __init__(self, embeddings, hidden_size, num_layers, num_classes, bidirectional, dropout_prob=0.3):\n",
    "#         super().__init__()\n",
    "#         self.embedding_layer = self.load_pretrained_embeddings(embeddings)\n",
    "\n",
    "#         self.dropout = nn.Dropout(p=dropout_prob)  # prob of an elt to be zeroed\n",
    "        \n",
    "#         self.lstm = nn.LSTM(300, hidden_size, num_layers, \\\n",
    "#                             dropout=dropout_prob, bidirectional=True, batch_first=True)\n",
    "#         self.non_linearity = nn.LeakyReLU()  # For example, ReLU\n",
    "        \n",
    "#         self.directions = 2\n",
    "#         self.linear_first = nn.Linear(hidden_size * self.directions * 3, hidden_size)  # why 4?\n",
    "#         self.linear_second = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.linear_third = nn.Linear(hidden_size, 300)\n",
    "                                      \n",
    "#         for layer in [self.linear_first, self.linear_second, self.linear_third]:\n",
    "#             nn.init.xavier_uniform_(layer.weight)\n",
    "#             nn.init.zeros_(layer.bias)\n",
    "\n",
    "#         self.clf = nn.Sequential(self.linear_first, self.non_linearity, self.dropout, \\\n",
    "#                                  self.linear_second, self.non_linearity, self.dropout, \\\n",
    "#                                  self.linear_third)         # classifier layer\n",
    "        \n",
    "\n",
    "#         \"\"\"\n",
    "#            Define the components of your BiLSTM Classifier model\n",
    "#            2. TODO: Your code here\n",
    "#         \"\"\"\n",
    "    \n",
    "# #         raise NotImplementedError  # delete this line\n",
    "    \n",
    "#     def load_pretrained_embeddings(self, embeddings):\n",
    "#         \"\"\"\n",
    "#            The code for loading embeddings from Lab 3 Deep Learning\n",
    "#            Unlike lab, we are not setting `embedding_layer.weight.requires_grad = False`\n",
    "#            because we want to finetune the embeddings on our data\n",
    "#         \"\"\"\n",
    "#         embedding_layer = nn.Embedding(embeddings.shape[0], embeddings.shape[1], padding_idx=0)\n",
    "#         embedding_layer.weight.data = torch.Tensor(embeddings).float()\n",
    "#         return embedding_layer\n",
    "\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         logits = None\n",
    "#         \"\"\"\n",
    "#            Write forward pass for LSTM. You must use dropout after embedding\n",
    "#            the inputs. \n",
    "\n",
    "#            Example, forward := embedding -> bilstm -> pooling (sum?mean?max?) \n",
    "#                               nonlinearity -> classifier\n",
    "#            Refer to: https://arxiv.org/abs/1705.02364\n",
    "\n",
    "#            Return logits\n",
    "\n",
    "#            3. TODO: Your code here\n",
    "#         \"\"\"\n",
    "#         inputs_embed = self.embedding_layer(inputs)\n",
    "        \n",
    "#         inputs_out, (inputs_ht, _) = self.lstm(inputs_embed, None)\n",
    "        \n",
    "#         inputs_f = inputs_out[:, -1, :]\n",
    "        \n",
    "#         logits = self.clf(inputs_f)\n",
    "        \n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6P7O47Lfr0a"
   },
   "source": [
    "First, we will define an evaluation function that will return the accuracy of the model. We will use this to compute validation accuracy and test accuracy of the model given a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dSkdBx1ULCFK"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    accuracy = None\n",
    "    model.eval()\n",
    "    \"\"\"\n",
    "        4. TODO: Your code here\n",
    "        Calculate the accuracy of the model on the data in dataloader\n",
    "        You may refer to `run_inference` function from Lab 3 part 1.\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_preds = []\n",
    "        for batch_text, batch_labels in dataloader:\n",
    "            preds = model(batch_text.to(device))\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "    preds = np.concatenate(all_preds, axis=0)\n",
    "    \n",
    "    \n",
    "    accuracy = test_labels==preds.argmax(-1).mean()\n",
    "    \n",
    "    return accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpCXZCO7iuEL"
   },
   "source": [
    "# Initialize the BiLSTM classifier model, criterion and optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "YaPW_CjlK0F7",
    "outputId": "60b93196-4c04-447c-c964-46f96a61834b",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-442b7e68d766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# BiLSTM hyperparameters\n",
    "hidden_size = 32\n",
    "num_layers = 1\n",
    "num_classes = 2\n",
    "bidirectional=True\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# if cuda exists, use cuda, else run on cpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = LSTMClassifier(embeddings, hidden_size, num_layers, num_classes, bidirectional)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1f8a688cae5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(data_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwkHYzbQikT4"
   },
   "source": [
    "# Train model with early stopping (10 pts)\n",
    "\n",
    "Train the model for `NUM_EPOCHS`. \n",
    "Keep track of training loss.  \n",
    "Compute the validation accuracy after each epoch. Keep track of the best validation accuracy and save the model with the best validation accuracy.  \n",
    "\n",
    "If the validation accuracy does not improve for more than `early_stop_patience` number of epochs in a row, stop training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290,
     "referenced_widgets": [
      "ffa4694843e146bfa077ff6ac50cdb41",
      "a183c3ce4b4c45df9dce8b95ba76be91",
      "02c9fe66444f428ab4408721a83cc973",
      "03ab8514b8e14283949ffbcef2562c91",
      "a49169acbd3e43b9a3590a82e68a9c1f",
      "a4d697719c0147488eb9aa66ab63aba3",
      "b250f8be29fd48c481a9586a036041d2",
      "ad486b7ed7cf4d078113cb68c0fa40ef"
     ]
    },
    "id": "QOlop_TMOD9V",
    "outputId": "a69e0251-3c4a-408a-ce89-0c2e845a6d44"
   },
   "outputs": [],
   "source": [
    "train_loss_history = []\n",
    "val_accuracy_history = []\n",
    "best_val_accuracy = 0\n",
    "n_no_improve = 0\n",
    "early_stop_patience=2\n",
    "NUM_EPOCHS=10\n",
    "  \n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    model.train()  # this enables dropout/regularization\n",
    "    for i, (data_batch, batch_labels) in enumerate(train_loader):\n",
    "        \"\"\"\n",
    "           Code for training lstm\n",
    "           Keep track of training of for each batch using train_loss_history\n",
    "        \"\"\"\n",
    "        preds = model(data_batch.to(device))\n",
    "        loss = criterion(preds, batch_labels.to(device))\n",
    "        \"\"\"\n",
    "          5(1). TODO: Recall that pytorch training involves five critical\n",
    "          components, as discussed in the Lab. Some of the components are\n",
    "          still missing here. Your code here.\n",
    "        \"\"\"\n",
    "#         raise NotImplementedError  # delete this line\n",
    "        train_loss_history.append(loss.item())\n",
    "        \n",
    "    # The end of a training epoch \n",
    "\n",
    "    \"\"\"\n",
    "        Code for tracking best validation accuracy, saving the best model, and early stopping\n",
    "        # Compute validation accuracy after each training epoch using `evaluate` function\n",
    "        # Keep track of validation accuracy in `val_accuracy_history`\n",
    "        # save model with best validation accuracy, hint: torch.save(model, 'best_model.pt')\n",
    "        # Early stopping: \n",
    "        # stop training if the validation accuracy does not improve for more than `early_stop_patience` runs\n",
    "        5(2). TODO: Your code here\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "#     raise NotImplementedError  # delete this line\n",
    "\n",
    "print(\"Best validation accuracy is: \", best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hhFpkMHnT7Z"
   },
   "source": [
    "#Question: Why do we want to use early stopping? Write the most important reason in concise way. (10 pts)\n",
    "\n",
    "Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I58rTeMEg05M"
   },
   "source": [
    "# Draw training curve \n",
    "X-axis: training steps, Y-axis: training loss\n",
    "\n",
    "Make sure to draw your own curves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "KyalZo6tSXo_",
    "outputId": "58d0c42f-f8d0-4bc2-f166-1777b2f5379b"
   },
   "outputs": [],
   "source": [
    "pd.Series(train_loss_history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMiI_u4ggvQK"
   },
   "source": [
    "# Validation accuracy curve\n",
    "X-axis: Epochs, Y-axis: validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "Qt8iNjFwPVtc",
    "outputId": "c721506d-b96b-4488-dcf1-0b91a86fe51f"
   },
   "outputs": [],
   "source": [
    "pd.Series(val_accuracy_history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqanG00Ggluj"
   },
   "source": [
    "## You should expect to get test accuracy > 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "cw6KtE2uSf1X",
    "outputId": "80b32fb5-10bf-4104-c8d5-eb5f3f53e83f"
   },
   "outputs": [],
   "source": [
    "# Reload best model from saved checkpoint\n",
    "# Compute test accuracy\n",
    "model = torch.load('best_model.pt')\n",
    "test_accuracy = evaluate(model, test_loader, device)\n",
    "print(test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DSGA1012-HW2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "010317c9128449e1b0b7b536b6091576": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "02c9fe66444f428ab4408721a83cc973": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4d697719c0147488eb9aa66ab63aba3",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a49169acbd3e43b9a3590a82e68a9c1f",
      "value": 8
     }
    },
    "03ab8514b8e14283949ffbcef2562c91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad486b7ed7cf4d078113cb68c0fa40ef",
      "placeholder": "​",
      "style": "IPY_MODEL_b250f8be29fd48c481a9586a036041d2",
      "value": " 80% 8/10 [02:37&lt;00:39, 19.65s/it]"
     }
    },
    "171f4eaed9bd4123ae3b3463a8a8b642": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18454ca7c2d749cbac54ade9cc8a1ce3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25e171acd5a84aa8b4de8aee01e3ccbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c9b2d4bfaa34f69a180b4bc948f014e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2f5b52b014f2439b905fa700bf636517": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "306699c7eb094435a950180873effe60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "39ad225c55a7418a8393e5d5217392f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d7fdd472f9d4e1a94f8afedfdb69dda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "424e07bfbd71417dbff02a70b4774aeb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48fbfa8e945f4e0989be39b6ff0247fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e3506a73adc4921bf8b2b110eb7a30a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bbd525ba97374d278314653168c08846",
      "max": 835,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3d7fdd472f9d4e1a94f8afedfdb69dda",
      "value": 835
     }
    },
    "5f4c4954e0c940c381d6aba07b72d67b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_819d692e3f61490e95bf3d289138ca85",
      "placeholder": "​",
      "style": "IPY_MODEL_ccc80287cfb84c4e88600fd970cb3a08",
      "value": "100%"
     }
    },
    "658dc5d1efc840cba2450740eaac9773": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c62fce4a8295479c83a928b79db89861",
       "IPY_MODEL_e335b3c83bc1446f9e690ef5aa19bccf",
       "IPY_MODEL_75e7a76026cd48e7b2b0575adff5395e"
      ],
      "layout": "IPY_MODEL_2f5b52b014f2439b905fa700bf636517"
     }
    },
    "66d27ecd0c12421c8295182d4b1c343d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f5cd07efca614a8586ea13a82f109290",
       "IPY_MODEL_e39a005b3c1f4f00a7ce74a023bb6fa9",
       "IPY_MODEL_6c2c857c8a8b4e29a09f713a83771f0d"
      ],
      "layout": "IPY_MODEL_171f4eaed9bd4123ae3b3463a8a8b642"
     }
    },
    "69ef9d2c6b5b492db9e33e48b0193f37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c2c857c8a8b4e29a09f713a83771f0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69ef9d2c6b5b492db9e33e48b0193f37",
      "placeholder": "​",
      "style": "IPY_MODEL_2c9b2d4bfaa34f69a180b4bc948f014e",
      "value": " 3902/3902 [00:02&lt;00:00, 1577.05it/s]"
     }
    },
    "70ede22c755a49d28e1803b112b973b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39ad225c55a7418a8393e5d5217392f4",
      "placeholder": "​",
      "style": "IPY_MODEL_a8028e07b39743f699237568579affb3",
      "value": " 835/835 [00:00&lt;00:00, 1202.05it/s]"
     }
    },
    "75e7a76026cd48e7b2b0575adff5395e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_424e07bfbd71417dbff02a70b4774aeb",
      "placeholder": "​",
      "style": "IPY_MODEL_48fbfa8e945f4e0989be39b6ff0247fc",
      "value": " 835/835 [00:00&lt;00:00, 989.08it/s]"
     }
    },
    "819d692e3f61490e95bf3d289138ca85": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8753674ef7f64029aae6f34d6e933f39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c817714e1a446dfb84717a18d82cbae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5f4c4954e0c940c381d6aba07b72d67b",
       "IPY_MODEL_5e3506a73adc4921bf8b2b110eb7a30a",
       "IPY_MODEL_70ede22c755a49d28e1803b112b973b8"
      ],
      "layout": "IPY_MODEL_97cdbcc74cb24fb9a4ec08187b08e019"
     }
    },
    "97cdbcc74cb24fb9a4ec08187b08e019": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a183c3ce4b4c45df9dce8b95ba76be91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a49169acbd3e43b9a3590a82e68a9c1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a4d697719c0147488eb9aa66ab63aba3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8028e07b39743f699237568579affb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8a08f68d96046c088a55f5e3863f47b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad486b7ed7cf4d078113cb68c0fa40ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b250f8be29fd48c481a9586a036041d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b3483370c5c541b9ae710e0e576ad7a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bbd525ba97374d278314653168c08846": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c62fce4a8295479c83a928b79db89861": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18454ca7c2d749cbac54ade9cc8a1ce3",
      "placeholder": "​",
      "style": "IPY_MODEL_8753674ef7f64029aae6f34d6e933f39",
      "value": "100%"
     }
    },
    "ccc80287cfb84c4e88600fd970cb3a08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e335b3c83bc1446f9e690ef5aa19bccf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6ea322a86ac4e70a57834232dfa6bab",
      "max": 835,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_306699c7eb094435a950180873effe60",
      "value": 835
     }
    },
    "e39a005b3c1f4f00a7ce74a023bb6fa9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25e171acd5a84aa8b4de8aee01e3ccbd",
      "max": 3902,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b3483370c5c541b9ae710e0e576ad7a8",
      "value": 3902
     }
    },
    "f5cd07efca614a8586ea13a82f109290": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_010317c9128449e1b0b7b536b6091576",
      "placeholder": "​",
      "style": "IPY_MODEL_a8a08f68d96046c088a55f5e3863f47b",
      "value": "100%"
     }
    },
    "f6ea322a86ac4e70a57834232dfa6bab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffa4694843e146bfa077ff6ac50cdb41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_02c9fe66444f428ab4408721a83cc973",
       "IPY_MODEL_03ab8514b8e14283949ffbcef2562c91"
      ],
      "layout": "IPY_MODEL_a183c3ce4b4c45df9dce8b95ba76be91"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
