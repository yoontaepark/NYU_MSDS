{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5mRDji1iNsc"
   },
   "source": [
    "# Fall 2022: DS-GA 1011 NLP with Representation Learning\n",
    "## Homework 2\n",
    "## Part 3: Neural Machine Translation (30 pts)\n",
    "In this part, you implement Transformer encoder for Neural Machine Translation (NMT) using a sequence to sequence (seq2seq) model for English to French translation with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki8Rdu4IiNsd"
   },
   "source": [
    "---\n",
    "### 1 Transformer Encoder (18 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OB4991PPiNse"
   },
   "outputs": [],
   "source": [
    "# Add utilities path\n",
    "import sys\n",
    "\n",
    "path_to_utils = 'pyfiles'\n",
    "sys.path.append(path_to_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1h61HxKEiNsi"
   },
   "outputs": [],
   "source": [
    "# Import custom modules\n",
    "import global_variables\n",
    "import nmt_dataset\n",
    "import nnet_models_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "juHsWAmriNsl"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "import os\n",
    "\n",
    "source_name = 'en'\n",
    "target_name = 'fr'\n",
    "\n",
    "base_saved_models_dir = '.'\n",
    "saved_models_dir = os.path.join(base_saved_models_dir, source_name+'2'+target_name)\n",
    "\n",
    "main_data_path = './data/'\n",
    "\n",
    "path_to_train_data = {'source':main_data_path+'train.'+source_name, \n",
    "                      'target':main_data_path+'train.'+target_name}\n",
    "path_to_val_data = {'source': main_data_path+'valid.'+source_name, \n",
    "                      'target':main_data_path+'valid.'+target_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4ZISJFayiNso"
   },
   "outputs": [],
   "source": [
    "saved_language_model_dir = os.path.join(saved_models_dir, 'lang_obj')\n",
    "\n",
    "dataset_dict = {'train': nmt_dataset.LanguagePair(source_name = source_name, target_name=target_name, \n",
    "                    filepath = path_to_train_data, \n",
    "                    lang_obj_path = saved_language_model_dir,\n",
    "                     minimum_count = 1), \n",
    "\n",
    "                'val': nmt_dataset.LanguagePair(source_name = source_name, target_name=target_name, \n",
    "                    filepath = path_to_val_data, \n",
    "                    lang_obj_path = saved_language_model_dir,\n",
    "                    minimum_count = 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['train'].main_df['source_len'].quantile(0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9F04tskaiNsr"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = int(dataset_dict['train'].main_df['source_len'].quantile(0.9999)) # 32\n",
    "batchSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-5GP1oyqiNsv"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader_dict = {'train': DataLoader(dataset_dict['train'], batch_size = batchSize, \n",
    "                            collate_fn = partial(nmt_dataset.vocab_collate_func, MAX_LEN=MAX_LEN),\n",
    "                            shuffle = True, num_workers=0), \n",
    "                    'val': DataLoader(dataset_dict['val'], batch_size = batchSize, \n",
    "                            collate_fn = partial(nmt_dataset.vocab_collate_func, MAX_LEN=MAX_LEN),\n",
    "                            shuffle = True, num_workers=0) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "l_T7Fi87iNsy"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "source_lang_obj = dataset_dict['train'].source_lang_obj\n",
    "target_lang_obj = dataset_dict['train'].target_lang_obj\n",
    "\n",
    "source_vocab = dataset_dict['train'].source_lang_obj.n_words;\n",
    "target_vocab = dataset_dict['train'].target_lang_obj.n_words;\n",
    "hidden_size = 512\n",
    "enc_layers = 1\n",
    "lr = 0.25;\n",
    "longest_label = 1;\n",
    "gradient_clip = 0.3;\n",
    "use_cuda = True\n",
    "\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bghnW3YiNs2"
   },
   "source": [
    "#### 1.1 Encoder (9 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4T23W8liNs4"
   },
   "outputs": [],
   "source": [
    "# Add transformer as encoder in seq2seq model\n",
    "\n",
    "# code below can help you to start it, but feel free to start from scratch\n",
    "\n",
    "class EncoderTransformer(nn.Module):\n",
    "     def __init__(self):\n",
    "        \n",
    "        # you need to add more things here\n",
    "        \n",
    "        self.position_embed = # sinusoidal embedding\n",
    "        encoder_layer = nn.TransformerEncoderLayer()\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "\n",
    "    def forward(self, text_vec):\n",
    "        # some helpful directions below, check the MLM lab for more details\n",
    "        \n",
    "        # embedded = pos_embedded + embedded  # apply pos embedding\n",
    "        # output = self.transformer(embedded)\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcUYkz_RiNs9"
   },
   "source": [
    "#### 1.2 Decoder(s) (9 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OT_JZXWeiNs9"
   },
   "outputs": [],
   "source": [
    "# Basic RNN decoder (no attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6DmOcLViNtB"
   },
   "outputs": [],
   "source": [
    "# RNN Decoder with Encoder attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szrb9vkiiNtE"
   },
   "outputs": [],
   "source": [
    "# RNN Decoder with Encoder & Self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_dYn8C_iNtH"
   },
   "source": [
    "#### Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1-h6PlgiNtI"
   },
   "outputs": [],
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLef3aD5iNtM"
   },
   "source": [
    "---\n",
    "### 2 Attention visualization (12 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVcysNCtiNtN"
   },
   "outputs": [],
   "source": [
    "# Model was trained in ~2 hours, i.e. you can expect attention maps\n",
    "# to look quite 'hard' (less soft spreading) i.e. attending to some particular token in the input"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw2-part3-nmt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
